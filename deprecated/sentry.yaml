repositories:
  # Stable repo of official helm charts
  - name: "stable"
    url: "https://charts.helm.sh/stable"
  # Cloud Posse incubator repo of helm charts
  - name: "cloudposse-incubator"
    url: "https://charts.cloudposse.com/incubator/"

releases:

  #
  # References:
  #   - https://github.com/getsentry/sentry
  #   - https://github.com/helm/charts/blob/master/stable/sentry/values.yaml
  #

  - name: "sentry"
    namespace: '{{ env "SENTRY_NAMESPACE" | default "sentry" }}'
    labels:
      chart: "sentry"
      repo: "stable"
      component: "monitoring"
      namespace: "sentry"
      vendor: "sentry"
    chart: "stable/sentry"
    version: '{{ env "SENTRY_CHART_VERSION" | default "4.0.0" }}'
    force: true
    wait: true
    timeout: 600
    installed: {{ env "SENTRY_INSTALLED" | default "true" }}
    values:
      - image:
          repository: '{{ env "SENTRY_IMAGE_REPOSITORY" | default "sentry" }}'
          tag: '{{ env "SENTRY_IMAGE_TAG" | default "9.1.1" }}'
          pullPolicy: "IfNotPresent"

        # https://github.com/helm/charts/blob/master/stable/sentry/templates/secrets.yaml#L12
        sentrySecret: '{{ env "SENTRY_SECRET" }}'

        web:
          replicacount: '{{ env "SENTRY_WEB_REPLICA_COUNT" | default "1" }}'
          resources:
            limits:
              cpu: '{{ env "SENTRY_WEB_RESOURCES_LIMIT_CPU" | default "200m" }}'
              memory: '{{ env "SENTRY_WEB_RESOURCES_LIMIT_MEMORY" | default "512Mi" }}'
            requests:
              cpu: '{{ env "SENTRY_WEB_RESOURCES_REQUEST_CPU" | default "100m" }}'
              memory: '{{ env "SENTRY_WEB_RESOURCES_REQUEST_MEMORY" | default "128Mi" }}'
          nodeSelector: {}
          tolerations: []
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - sentry
                        - key: role
                          operator: In
                          values:
                            - web
                    topologyKey: "kubernetes.io/hostname"
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          readinessProbe:
            failureThreshold: 10
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          env:
            - name: SENTRY_SLACK_CLIENT_ID
              value: '{{ env "SENTRY_SLACK_CLIENT_ID" | default "" }}'
            - name: SENTRY_SLACK_CLIENT_SECRET
              value: '{{ env "SENTRY_SLACK_CLIENT_SECRET" | default "" }}'
            - name: SENTRY_SLACK_VERIFICATION_TOKEN
              value: '{{ env "SENTRY_SLACK_VERIFICATION_TOKEN" | default "" }}'

          ## Use an alternate scheduler, e.g. "stork".
          ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
          ##
          # schedulerName:
          # Optional extra labels for pod, i.e. redis-client: "true"
          # podLabels: []

        cron:
          replicacount: '{{ env "SENTRY_CRON_REPLICA_COUNT" | default "1" }}'
          resources:
            limits:
              cpu: '{{ env "SENTRY_CRON_RESOURCES_LIMIT_CPU" | default "200m" }}'
              memory: '{{ env "SENTRY_CRON_RESOURCES_LIMIT_MEMORY" | default "256Mi" }}'
            requests:
              cpu: '{{ env "SENTRY_CRON_RESOURCES_REQUEST_CPU" | default "100m" }}'
              memory: '{{ env "SENTRY_CRON_RESOURCES_REQUEST_MEMORY" | default "128Mi" }}'
          nodeSelector: {}
          tolerations: []
          affinity: {}
          env:
            - name: SENTRY_SLACK_CLIENT_ID
              value: '{{ env "SENTRY_SLACK_CLIENT_ID" | default "" }}'
            - name: SENTRY_SLACK_CLIENT_SECRET
              value: '{{ env "SENTRY_SLACK_CLIENT_SECRET" | default "" }}'
            - name: SENTRY_SLACK_VERIFICATION_TOKEN
              value: '{{ env "SENTRY_SLACK_VERIFICATION_TOKEN" | default "" }}'
          # Optional extra labels for pod, i.e. redis-client: "true"
          # podLabels: []

        worker:
          # See https://docs.sentry.io/server/performance/ about number of workers and concurrency
          replicacount: '{{ env "SENTRY_WORKER_REPLICA_COUNT" | default "2" }}'
          # Recommended currency is 4 or fewer
          concurrency: '{{ env "SENTRY_WORKER_CONCURRENCY" | default "4" }}'
          resources:
            limits:
              cpu: '{{ env "SENTRY_WORKER_RESOURCES_LIMIT_CPU" | default "300m" }}'
              memory: '{{ env "SENTRY_WORKER_RESOURCES_LIMIT_MEMORY" | default "512Mi" }}'
            requests:
              cpu: '{{ env "SENTRY_WORKER_RESOURCES_REQUEST_CPU" | default "100m" }}'
              memory: '{{ env "SENTRY_WORKER_RESOURCES_REQUEST_MEMORY" | default "256Mi" }}'
          nodeSelector: {}
          tolerations: []
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchExpressions:
                        - key: app
                          operator: In
                          values:
                            - sentry
                        - key: role
                          operator: In
                          values:
                            - worker
                    topologyKey: "kubernetes.io/hostname"
          env:
            - name: SENTRY_SLACK_CLIENT_ID
              value: '{{ env "SENTRY_SLACK_CLIENT_ID" | default "" }}'
            - name: SENTRY_SLACK_CLIENT_SECRET
              value: '{{ env "SENTRY_SLACK_CLIENT_SECRET" | default "" }}'
            - name: SENTRY_SLACK_VERIFICATION_TOKEN
              value: '{{ env "SENTRY_SLACK_VERIFICATION_TOKEN" | default "" }}'
          # Optional extra labels for pod, i.e. redis-client: "true"
          # podLabels: []
          # concurrency:

        # Admin user to create
        # https://github.com/helm/charts/blob/master/stable/sentry/templates/secrets.yaml
        user:
          # Indicated to create the admin user or not,
          # Default is true at the initial installation.
          create: {{ env "SENTRY_CREATE_ADMIN_USER" | default "true" }}
          email: '{{ env "SENTRY_ADMIN_USER_EMAIL" }}'
          password: '{{ env "SENTRY_ADMIN_USER_PASSWORD" }}'

        # BYO Email server
        # https://docs.sentry.io/server/installation/docker/#outbound-email
        email:
          from_address: '{{ env "SENTRY_EMAIL_FROM_ADDRESS" | default "" }}'
          host: '{{ env "SENTRY_EMAIL_HOST" | default "smtp" }}'
          port: '{{ env "SENTRY_EMAIL_PORT" | default "25" }}'
          use_tls: {{ env "SENTRY_EMAIL_USE_TLS" | default "false" }}
          user: '{{ env "SENTRY_EMAIL_USER" | default "" }}'
          password: '{{ env "SENTRY_EMAIL_PASSWORD" | default "" }}'
          enable_replies: {{ env "SENTRY_EMAIL_ENABLE_REPLIES" | default "false" }}

        service:
          name: sentry
          type: ClusterIP
          externalPort: '{{ env "SENTRY_SERVICE_EXTERNAL_PORT" | default "9000" }}'
          internalPort: '{{ env "SENTRY_SERVICE_INTERNAL_PORT" | default "9000" }}'
          annotations: {}
          labels:
            # Show service URL in `kubectl cluster-info`
            kubernetes.io/cluster-service: "true"

          # type: LoadBalancer
          # externalPort: 9000
          # internalPort: 9000

          ## External IP addresses of service
          ## Default: nil
          ##
          # externalIPs:
          # - 192.168.0.1

          ## Load Balancer allow-list
          # loadBalancerSourceRanges: []

        # Configure the location of Sentry artifacts
        filestore:
          # Set to one of filesystem, gcs or s3 as supported by Sentry.
          # S3 will be preferable once the chart supports a default ACL
          # and getting credentials from the metadata server.
          # See https://github.com/getsentry/sentry-docs/pull/956/files
          backend: filesystem

          filesystem:
            path: /var/lib/sentry/files

            ## Enable persistence using Persistent Volume Claims
            ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
            ##
            persistence:
              enabled: {{ env "SENTRY_FILESTORE_PERSISTENCE_ENABLED" | default "true" }}
              ## If defined, storageClassName: <storageClass>
              ## If set to "-", storageClassName: "", which disables dynamic provisioning
              ## If undefined (the default) or set to null, no storageClassName spec is
              ## set, choosing the default provisioner.  (gp2 on AWS, standard on
              ## GKE, AWS & OpenStack)
              storageClass: '{{ env "SENTRY_FILESTORE_PERSISTENCE_STORAGE_CLASS" | default "gp2" }}'
              accessMode: '{{ env "SENTRY_FILESTORE_PERSISTENCE_ACCESS_MODE" | default "ReadWriteOnce" }}'
              size: '{{ env "SENTRY_FILESTORE_PERSISTENCE_SIZE" | default "10Gi" }}'

              ## Whether to mount the persistent volume to the Sentry worker and
              ## cron deployments. This setting needs to be enabled for some advanced
              ## Sentry features, such as private source maps. If you disable this
              ## setting, the Sentry workers will not have access to artifacts you upload
              ## through the web deployment.
              ## Please note that you may need to change your accessMode to ReadWriteMany
              ## if you plan on having the web, worker and cron deployments run on
              ## different nodes.
              persistentWorkers: {{ env "SENTRY_FILESTORE_WORKERS_PERSISTENCE_ENABLED" | default "false" }}

          ## Point this at a pre-configured secret containing a service account. The resulting
          ## secret will be mounted at /var/run/secrets/google
          #gcs:
          #  credentialsFile: credentials.json
          #  secretName:
          #  bucketName:

          ## Currently unconfigured and changing this has no impact on the template configuration.
          #s3: {}
          #  accessKey:
          #  secretKey:
          #  bucketName:

        ## Configure ingress resource that allow you to access the
        ## Sentry installation. Set up the URL
        ## ref: http://kubernetes.io/docs/user-guide/ingress/
        ##
        ingress:
          enabled: {{ env "SENTRY_INGRESS_ENABLED" | default "false" }}
          hostname: '{{ env "SENTRY_HOSTNAME" }}'
          annotations:
            kubernetes.io/ingress.class: "nginx"
            kubernetes.io/tls-acme: "true"
            external-dns.alpha.kubernetes.io/ttl: "60"
            external-dns.alpha.kubernetes.io/target: '{{ env "SENTRY_INGRESS" }}'
            external-dns.alpha.kubernetes.io/hostname: '{{ env "SENTRY_HOSTNAME" }}'
          tls:
            - secretName: '{{ env "SENTRY_TLS_SECRET_NAME" | default "sentry-server-tls" }}'
              hosts:
                - '{{ env "SENTRY_HOSTNAME" }}'

        postgresql:
          enabled: {{ env "SENTRY_POSTGRES_ENABLED" | default "false" }}
          imageTag: '{{ env "SENTRY_POSTGRES_IMAGE_TAG" | default "9.6" }}'
          persistence:
            enabled: {{ env "SENTRY_POSTGRES_PERSISTENCE_ENABLED" | default "false" }}
          postgresqlDatabase: '{{ env "SENTRY_POSTGRES_DATABASE" | default "sentry" }}'
          postgresqlUsername: '{{ env "SENTRY_POSTGRES_USER" | default "sentry" }}'
          # Only used when internal PG is disabled
          postgresqlHost: '{{ env "SENTRY_POSTGRES_HOST" | default "postgres" }}'
          postgresqlPassword: '{{ env "SENTRY_POSTGRES_PASSWORD" | default "" }}'
          postgresqlPort: '{{ env "SENTRY_POSTGRES_PORT" | default "5432" }}'

        redis:
          enabled: {{ env "SENTRY_REDIS_ENABLED" | default "false" }}
          # Only used when internal redis is disabled
          host: '{{ env "SENTRY_REDIS_HOST" | default "redis" }}'
          # Just omit the password field if your redis cluster doesn't use password
          password: '{{ env "SENTRY_REDIS_PASSWORD" | default "" }}'
          port: '{{ env "SENTRY_REDIS_PORT" | default "6379" }}'
          master:
            persistence:
              enabled: {{ env "SENTRY_REDIS_MASTER_PERSISTENCE_ENABLED" | default "false" }}
              size: '{{ env "SENTRY_REDIS_MASTER_PERSISTENCE_SIZE" | default "20Gi" }}'

        # https://docs.sentry.io/server/config/
        config:
          configYml: |-
            system.url-prefix: 'https://{{ env "SENTRY_HOSTNAME" }}'
            github.apps-install-url: 'https://{{ env "SENTRY_HOSTNAME" }}/extensions/github/setup/'
            github-app.id: {{ env "SENTRY_GITHUB_APP_ID" | default "" }}
            github-app.client-id: '{{ env "SENTRY_GITHUB_APP_CLIENT_ID" | default "" }}'
            github-app.name: '{{ env "SENTRY_GITHUB_APP_NAME" | default "" }}'
            github-app.client-secret: '{{ env "SENTRY_GITHUB_APP_CLIENT_SECRET" | default "" }}'
            github-app.webhook-secret: '{{ env "SENTRY_GITHUB_APP_WEBHOOK_SECRET" | default "" }}'
            github-app.private-key: |-
{{ env "SENTRY_GITHUB_APP_PRIVATE_KEY" | default "" | indent 14 }}

          sentryConfPy: |
            SLACK_INTEGRATION_USE_WST = False

        ## Prometheus Exporter / Metrics
        ##
        metrics:
          enabled: {{ env "SENTRY_METRICS_ENABLED" | default "false" }}

          ## Configure extra options for liveness and readiness probes
          ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)
          livenessProbe:
            enabled: true
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 2
            failureThreshold: 3
            successThreshold: 1
          readinessProbe:
            enabled: true
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 2
            failureThreshold: 3
            successThreshold: 1

          ## Metrics exporter resource requests and limits
          ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
          resources:
            limits:
              cpu: '{{ env "SENTRY_METRICS_RESOURCES_LIMIT_CPU" | default "50m" }}'
              memory: '{{ env "SENTRY_METRICS_RESOURCES_LIMIT_MEMORY" | default "24Mi" }}'
            requests:
              cpu: '{{ env "SENTRY_METRICS_RESOURCES_REQUEST_CPU" | default "50m" }}'
              memory: '{{ env "SENTRY_METRICS_RESOURCES_REQUEST_MEMORY" | default "12Mi" }}'

          nodeSelector: {}
          tolerations: []
          affinity: {}
          # schedulerName:
          # Optional extra labels for pod, i.e. redis-client: "true"
          # podLabels: []
          service:
            type: ClusterIP
            labels: {}

          image:
            repository: prom/statsd-exporter
            tag: '{{ env "SENTRY_METRICS_IMAGE_TAG" | default "v0.12.2" }}'
            pullPolicy: IfNotPresent

          # Enable this if you're using https://github.com/coreos/prometheus-operator
          serviceMonitor:
            enabled: {{ env "SENTRY_METRICS_SERVICE_MONITOR_ENABLED" | default "false" }}
            ## Specify a namespace if needed
            # namespace: monitoring
            # fallback to the prometheus default unless specified
            # interval: 10s
            ## Defaults to what's used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr)
            ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1)
            ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters)
            selector:
              prometheus: {{ env "SENTRY_PROMETHEUS_SELECTOR" | default "kube-prometheus" }}


        # Provide affinity for hooks if needed
        hooks:
          affinity: {}
          dbInit:
            resources:
              # We setup 3000Mi for the memory limit because of a Sentry instance need at least 3Gb RAM to perform a migration process
              # reference: https://github.com/helm/charts/issues/15296
              limits:
                memory: '{{ env "SENTRY_HOOKS_RESOURCES_LIMIT_MEMORY" | default "3200Mi" }}'
              requests:
                memory: '{{ env "SENTRY_HOOKS_RESOURCES_REQUEST_MEMORY" | default "3200Mi" }}'

  - name: 'sentry-alerts'
    chart: "cloudposse-incubator/monochart"
    version: "0.18.4"
    wait: true
    force: true
    installed: {{ env "SENTRY_ALERTS_INSTALLED" | default "true" }}
    values:
      - prometheusRules:
          name:
            labels:
              app: prometheus-operator-prometheus
            groups:
              - name: sentry.rules
                rules:
                  - alert: SentryQueueProcessingIdle
                    annotations:
                      message: Sentry events queue processing is idle
                      description: Sentry events queue processing has stalled. It is possible that sentry worker(s) are stuck and must be restarted by killing the sentry worker pods.
                      url:  https://{{ env "SENTRY_HOSTNAME" }}/manage/queue/
                    expr: (sentry_jobs_started - sentry_jobs_started offset 5m) == 0
                    for: 5m
                    labels:
                      severity: warning
                  - alert: SentryTimeToProcessHigh
                    annotations:
                      message: Sentry time to process events is elevated (currently is {{` {{ $value | printf "%.2f"}}s `}})
                    expr: sentry_events_time_to_process > 0.25
                    for: 5m
                    labels:
                      severity: error
                  - alert: SentryErrorRateHigh
                    annotations:
                      message: Sentry events error rate is elevated (currently is {{` {{ $value | printf "%.2f"}}% `}})
                    expr: increase(sentry_events_failed[5m]) / (increase(sentry_events_failed[5m]) + increase(sentry_events_processed[5m])) * 100 > 5
                    for: 5m
                    labels:
                      severity: error
                  - alert: Sentry5xxErrorRate
                    annotations:
                      message: Sentry client API is responding with 5xx status code (currently is {{` {{ $value | printf "%.2f"}}/s `}})
                    expr: rate(sentry_client_api_all_versions_responses_5xx[5m]) > 0
                    for: 15m
                    labels:
                      severity: error
