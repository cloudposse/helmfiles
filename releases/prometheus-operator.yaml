#####
# WARNING: This is a complete rewrite of this release as we switch from
# the deprecated coreos-stable chart at https://github.com/coreos/prometheus-operator/tree/master/helm/prometheus-operator
# to the current helm stable chart at https://github.com/helm/charts/tree/master/stable/prometheus-operator
#
# OLD References:
# OLD  - https://github.com/coreos/prometheus-operator/tree/master/helm/prometheus-operator
# OLD  - https://github.com/coreos/prometheus-operator
#
repositories:
  # Official helm charts
  - name: "stable"
    url: "https://kubernetes-charts.storage.googleapis.com/"

releases:

  #######################################################################################
  ## prometheus-operator                                                               ##
  ## creates/configures/manages Prometheus clusters atop Kubernetes                    ##
  ## This is the all-in-one version that                                               ##
  ## replaces https://github.com/coreos/prometheus-operator/tree/master/helm           ##
  #######################################################################################

  #
  # References:
  #   - https://github.com/helm/charts/tree/master/stable/prometheus-operator
  #   - https://github.com/coreos/prometheus-operator
  #
  - name: "prometheus-operator"
    namespace: "monitoring"
    labels:
      chart: "prometheus-operator"
      repo: "coreos-stable"
      component: "monitoring"
      namespace: "monitoring"
      vendor: "coreos"
      default: "true"
    chart: "stable/prometheus-operator"
    version: "5.5.0"
    wait: true
    installed: {{ env "PROMETHEUS_OPERATOR_INSTALLED" | default "true" }}
    hooks:
    # This hoook installs the prometheuses.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["prepare"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd prometheuses.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheus.crd.yaml"]
    # This hoook installs the alertmanagers.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["prepare"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd alertmanagers.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/alertmanager.crd.yaml"]
    # This hoook installs the prometheusrules.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["prepare"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd prometheusrules.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/prometheusrule.crd.yaml"]
    # This hoook installs the servicemonitors.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["prepare"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd servicemonitors.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/servicemonitor.crd.yaml"]
    values:
    - global:
        rbac:
          create: {{ env "RBAC_ENABLED" | default "true" }}
          pspEnabled: {{ coalesce (env "POD_SECURITY_POLICY_ENALBED") (env "RBAC_ENABLED") "true" }}
      defaultRules:
        create: true
      additionalPrometheusRules: []
      prometheusOperator:
        enabled: {{ env "PROMETHEUS_OPERATOR_INSTALLED" | default "true" }}
        ### Create CRDs separately
        createCustomResource: false
        # log level must be one of "all", "debug",	"info", "warn",	"error", "none"
        logLevel: "warn"
        resources:
          limits:
            cpu: '{{ env "PROMETHEUS_OPERATOR_LIMIT_CPU" | default "100m" }}'
            memory: '{{ env "PROMETHEUS_OPERATOR_LIMIT_MEMORY" | default "96Mi" }}'
          requests:
            cpu: '{{ env "PROMETHEUS_OPERATOR_REQUEST_CPU" | default "20m" }}'
            memory: '{{ env "PROMETHEUS_OPERATOR_REQUEST_MEMORY" | default "48Mi" }}'
        image:
          pullPolicy: "IfNotPresent"
      prometheus:
        enabled: {{ env "PROMETHEUS_OPERATOR_INSTALLED" | default "true" }}
        podDisruptionBudget:
          enabled: false
        ingress:
          enabled: false
        additionalServiceMonitors: []
        prometheusSpec:
          replicas: 1
          retention: 45d
          logLevel: "warn"
          scrapeInterval: ""
          evaluationInterval: ""
          ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
          ## prometheus resource to be created with selectors based on values in the helm deployment,
          ## which will also match the PrometheusRule resources created.
          ## If false, a nil or or {} value for ruleSelector will select all PrometheusRule resources.
          ruleSelectorNilUsesHelmValues: false
          ## serviceMonitorSelectorNilUsesHelmValues works just like ruleSelectorNilUsesHelmValues
          serviceMonitorSelectorNilUsesHelmValues: false
          externalUrl: "{{- env "PROMETHEUS_PROMETHEUS_EXTERNAL_URL" | default (print "https://api." (env "KOPS_CLUSTER_NAME") "/api/v1/namespaces/monitoring/services/prometheus-operator-prometheus:web/proxy/") }}"
          resources:
            limits:
              cpu: '{{ env "PROMETHEUS_PROMETHEUS_LIMIT_CPU" | default "300m" }}'
              memory: '{{ env "PROMETHEUS_PROMETHEUS_LIMIT_MEMORY" | default "1Gi" }}'
            requests:
              cpu: '{{ env "PROMETHEUS_PROMETHEUS_REQUEST_CPU" | default "50m" }}'
              memory: '{{ env "PROMETHEUS_PROMETHEUS_REQUEST_MEMORY" | default "512Mi" }}'
      alertmanager:
        enabled: {{ env "ALERTMANAGER_INSTALLED" | default "true" }}
        alertmanagerSpec:
          externalUrl: "{{- env "PROMETHEUS_ALERTMANAGER_EXTERNAL_URL" | default (print "https://api." (env "KOPS_CLUSTER_NAME") "/api/v1/namespaces/monitoring/services/prometheus-operator-alertmanager:web/proxy/") }}"
          resources:
            limits:
              cpu: '{{ env "PROMETHEUS_ALERTMANAGER_LIMIT_CPU" | default "200m" }}'
              memory: '{{ env "PROMETHEUS_ALERTMANAGER_LIMIT_MEMORY" | default "96Mi" }}'
            requests:
              cpu: '{{ env "PROMETHEUS_ALERTMANAGER_REQUEST_CPU" | default "10m" }}'
              memory: '{{ env "PROMETHEUS_ALERTMANAGER_REQUEST_MEMORY" | default "24Mi" }}'
      grafana:
        enabled: {{ env "GRAFANA_INSTALLED" | default "true" }}
        adminPassword: {{ env "GRAFANA_ADMIN_PASSWORD" | default "prom-operator" }}
        grafana.ini:
          server:
            # root_url: "https://api.{{- env "KOPS_CLUSTER_NAME" }}/api/v1/namespaces/kube-system/services/prometheus-operator-grafana:service/proxy/"
            root_url: "{{- env "PROMETHEUS_GRAFANA_ROOT_URL" | default (print "https://api." (env "KOPS_CLUSTER_NAME") "/api/v1/namespaces/monitoring/services/prometheus-operator-grafana:service/proxy/") }}"
          {{- if eq (env "PROMETHEUS_ANONYMOUS_ADMIN_ENABLED" | default "true") "true" }}
          auth.anonymous:
            enabled: true
            org_role: Admin
          {{- end }}
      kubeApiServer:
        enabled: true
      kubelet:
        enabled: true
        # In general, few clusters are set up to allow kublet to authenticate a bearer token, and
        # the HTTPS endpoint requires authentication, so Prometheus cannot access it.
        # The HTTP endpoint does not require authentication, so Prometheus can access it.
        # See https://github.com/coreos/prometheus-operator/issues/926
        https: {{ env "PROMETHEUS_KUBELET_HTTPS" | default "false" }}
      kubeControllerManager:
        enabled: true
        {{- if eq (env "PROMETHEUS_KUBE_K8S_APP_LABEL_NAME" | default "false") "true" }}
        service:
          selector:
            k8s-app: "kube-controller-manager"
            component: null
        {{- end }}
      coreDns:
        enabled: {{ coalesce (env "PROMETHEUS_EXPORTER_CORE_DNS_ENABLED") "true" }}
      kubeDns:
        enabled: {{ coalesce (env "PROMETHEUS_EXPORTER_KUBE_DNS_ENABLED") "false" }}
      kubeEtcd:
        enabled: true
      kubeScheduler:
        enabled: true
        {{- if eq (env "PROMETHEUS_KUBE_K8S_APP_LABEL_NAME" | default "false") "true" }}
        service:
          selector:
            k8s-app: "kube-scheduler"
            component: null
        {{- end }}
      nodeExporter:
        enabled: true
    {{- if env "PROMETHEUS_ADDITIONAL_CONFIG_YAML" }}
    - {{ env "PROMETHEUS_ADDITIONAL_CONFIG_YAML" }}
    {{ end }}
