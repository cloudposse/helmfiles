#####
# WARNING: This is a complete rewrite of this release as we switch
# to the
#
#        CURRENT helm stable chart at https://github.com/helm/charts/tree/master/stable/prometheus-operator
#
# from the DEPRECATED coreos-stable chart at
# https://github.com/coreos/prometheus-operator/tree/master/helm/prometheus-operator
#
# OLD References:
# OLD  - https://github.com/coreos/prometheus-operator/tree/master/helm/prometheus-operator
# OLD  - https://github.com/coreos/prometheus-operator
#
repositories:
  # Official helm charts
  - name: "stable"
    url: "https://kubernetes-charts.storage.googleapis.com/"

releases:

  #######################################################################################
  ## prometheus-operator                                                               ##
  ## creates/configures/manages Prometheus clusters atop Kubernetes                    ##
  ## This is the all-in-one version that                                               ##
  ## replaces https://github.com/coreos/prometheus-operator/tree/master/helm           ##
  #######################################################################################

  #
  # References:
  #   - https://github.com/helm/charts/tree/master/stable/prometheus-operator
  #   - https://github.com/coreos/prometheus-operator
  #
  - name: "prometheus-operator"
    namespace: "monitoring"
    labels:
      chart: "prometheus-operator"
      repo: "coreos-stable"
      component: "monitoring"
      namespace: "monitoring"
      vendor: "coreos"
      default: "true"
    chart: "stable/prometheus-operator"
    # https://github.com/helm/charts/tree/50fa3099e2f460f3aba04870b845b1b076a5ed8a/stable/prometheus-operator
    version: "6.2.1"
    wait: true
    installed: {{ env "PROMETHEUS_OPERATOR_INSTALLED" | default "true" }}
    hooks:
    # Hooks associated to presync events are triggered before each release is applied to the remote cluster.
    # This is the ideal event to execute any commands that may mutate the cluster state as it
    # will not be run for read-only operations like lint, diff or template.
    # These hoook install the prometheuses.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["presync"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd prometheuses.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml"]
    # This hoook installs the alertmanagers.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["presync"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd alertmanagers.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml"]
    # This hoook installs the prometheusrules.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["presync"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd prometheusrules.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml"]
    # This hoook installs the servicemonitors.monitoring.coreos.com CustomResourceDefinition if needed
    - events: ["presync"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd servicemonitors.monitoring.coreos.com >/dev/null 2>&1 || \
             kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml"]
    - events: ["presync"]
      command: "/bin/sh"
      args: ["-c", "kubectl get crd podmonitors.monitoring.coreos.com >/dev/null 2>&1 || \
          kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/master/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml"]
    values:
    - global:
        rbac:
          create: {{ env "RBAC_ENABLED" | default "true" }}
          pspEnabled: {{ coalesce (env "POD_SECURITY_POLICY_ENALBED") (env "RBAC_ENABLED") "true" }}
      defaultRules:
        create: true
      {{- if eq (env "PROMETHEUS_OPERATOR_RULES_KUBERNETES_RESOURCES_ALTERNATE_ENABLED" | default "false") "false" }}
        rules:
          # The default kubernetesResources rules trigger CPUThrottlingHigh alerts due to known bugs in
          # the Linux kernel. You may want to replace them with rules better tuned to your situation
          kubernetesResources: {{ env "PROMETHEUS_OPERATOR_RULES_KUBERNETES_RESOURCES_ENABLED" | default "true" }}
      additionalPrometheusRulesMap: {}
      {{- else }}
        rules:
          kubernetesResources: false
      additionalPrometheusRulesMap:
        # These rules are copied from https://raw.githubusercontent.com/coreos/kube-prometheus/release-0.1/manifests/prometheus-rules.yaml
        # CPUThrottlingHigh has been modified, to be replaced with a customizable version
        # to reduce alerts caused by https://github.com/kubernetes/kubernetes/pull/63437
        # ClockSkewDetected has been modified to fire less often, for changed from 2m to 10m
        kubernetes-resources:
          groups:
          - name: node-time
            rules:
            - alert: ClockSkewDetected
              annotations:
                message: Clock skew detected on node-exporter {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}}. Ensure NTP is configured correctly on this host.
              expr: abs(node_timex_offset_seconds{job="node-exporter"}) > 0.03
              for: 10m
              labels:
                severity: warning
          - name: kubernetes-resources
            rules:
            - alert: KubeCPUOvercommit
              annotations:
                message: Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure.
                runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
              expr: |-
                sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)
                  /
                sum(node:node_num_cpu:sum)
                  >
                (count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)
              for: 5m
              labels:
                severity: warning
            - alert: KubeMemOvercommit
              annotations:
                message: Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure.
                runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
              expr: |-
                sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)
                  /
                sum(node_memory_MemTotal_bytes)
                  >
                (count(node:node_num_cpu:sum)-1)
                  /
                count(node:node_num_cpu:sum)
              for: 5m
              labels:
                severity: warning
            - alert: KubeCPUOvercommit
              annotations:
                message: Cluster has overcommitted CPU resource requests for Namespaces.
                runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
              expr: |-
                sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
                  /
                sum(node:node_num_cpu:sum)
                  > 1.5
              for: 5m
              labels:
                severity: warning
            - alert: KubeMemOvercommit
              annotations:
                message: Cluster has overcommitted memory resource requests for Namespaces.
                runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
              expr: |-
                sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
                  /
                sum(node_memory_MemTotal_bytes{job="node-exporter"})
                  > 1.5
              for: 5m
              labels:
                severity: warning
            - alert: KubeQuotaExceeded
              annotations:
                message: Namespace {{`{{ $labels.namespace }}`}} is using {{`{{ printf "%0.0f" $value }}`}}% of its {{`{{ $labels.resource }}`}} quota.
                runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
              expr: |-
                100 * kube_resourcequota{job="kube-state-metrics", type="used"}
                  / ignoring(instance, job, type)
                (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
                  > 90
              for: 15m
              labels:
                severity: warning
            {{- if eq (env "PROMETHEUS_OPERATOR_RULES_CPU_THROTTLING_HIGH_ENABLED" | default "true") "true" }}
            # Original rule is 25% for 15 minutes
            - alert: CPUThrottlingHigh-{{- env "PROMETHEUS_OPERATOR_RULES_CPU_THROTTLING_HIGH_THRESHOLD_PERCENT" | default "50" -}}-{{- env "PROMETHEUS_OPERATOR_RULES_CPU_THROTTLING_HIGH_THRESHOLD_TIME" | default "25m" }}
              annotations:
                message: '{{`{{ printf "%0.0f" $value }}`}}% throttling of CPU in namespace {{`{{ $labels.namespace }}`}} for container {{`{{ $labels.container_name }}`}} in pod {{`{{ $labels.pod_name }}`}}.'
                runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh
              expr: |-
                100 * sum(increase(container_cpu_cfs_throttled_periods_total{container_name!="", }[5m])) by (container_name, pod_name, namespace)
                  /
                sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container_name, pod_name, namespace)
                  > {{ env "PROMETHEUS_OPERATOR_RULES_CPU_THROTTLING_HIGH_THRESHOLD_PERCENT" | default "50" }}
              for: {{ env "PROMETHEUS_OPERATOR_RULES_CPU_THROTTLING_HIGH_THRESHOLD_TIME" | default "25m" }}
              labels:
                severity: warning
            {{- end }}
      {{- end }}
      prometheusOperator:
        enabled: {{ env "PROMETHEUS_OPERATOR_INSTALLED" | default "true" }}
        ### Create CRDs separately
        createCustomResource: false
        # log level must be one of "all", "debug",	"info", "warn",	"error", "none"
        logLevel: "warn"
        resources:
          limits:
            cpu: '{{ env "PROMETHEUS_OPERATOR_LIMIT_CPU" | default "100m" }}'
            memory: '{{ env "PROMETHEUS_OPERATOR_LIMIT_MEMORY" | default "96Mi" }}'
          requests:
            cpu: '{{ env "PROMETHEUS_OPERATOR_REQUEST_CPU" | default "20m" }}'
            memory: '{{ env "PROMETHEUS_OPERATOR_REQUEST_MEMORY" | default "48Mi" }}'
        image:
          pullPolicy: "IfNotPresent"
      prometheus:
        enabled: {{ env "PROMETHEUS_OPERATOR_INSTALLED" | default "true" }}
        podDisruptionBudget:
          enabled: false
        ingress:
          enabled: false
        additionalServiceMonitors: []
        prometheusSpec:
          {{- if or (env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_REPOSITORY") (env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_TAG") }}
          image:
            {{- if env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_REPOSITORY" }}
            repository: {{ env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_REPOSITORY" }}
            {{- end }}
            {{- if env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_TAG" }}
            tag: {{ env "PROMETHEUS_OPERATOR_PROMETHEUS_IMAGE_TAG" }}
            {{- end }}
          {{- end }}
          replicas: 1
          retention: {{ env "PROMETHEUS_OPERATOR_PROMETHEUS_RETENTION_PERIOD" | default "45d" }}
          logLevel: "warn"
          podMetadata:
            annotations:
              "cluster-autoscaler.kubernetes.io/safe-to-evict": "true"
          scrapeInterval: ""
          evaluationInterval: ""
          ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
          ## prometheus resource to be created with selectors based on values in the helm deployment,
          ## which will also match the PrometheusRule resources created.
          ## If false, a nil or or {} value for ruleSelector will select all PrometheusRule resources.
          ruleSelectorNilUsesHelmValues: false
          ## serviceMonitorSelectorNilUsesHelmValues works just like ruleSelectorNilUsesHelmValues
          serviceMonitorSelectorNilUsesHelmValues: false
          {{- if env "PROMETHEUS_OPERATOR_PROMETHEUS_STORAGE_SIZE" }}
          storageSpec:
            volumeClaimTemplate:
              spec:
                {{- if env "PROMETHEUS_OPERATOR_PROMETHEUS_STORAGE_CLASS" }}
                storageClassName: {{ env "PROMETHEUS_OPERATOR_PROMETHEUS_STORAGE_CLASS" }}
                {{- end }}
                resources:
                  requests:
                    storage: {{ env "PROMETHEUS_OPERATOR_PROMETHEUS_STORAGE_SIZE" }}
          {{- end }}
          externalUrl: "{{- env "PROMETHEUS_PROMETHEUS_EXTERNAL_URL" | default (print "https://api." (env "KOPS_CLUSTER_NAME") "/api/v1/namespaces/monitoring/services/prometheus-operator-prometheus:web/proxy/") }}"
          resources:
            limits:
              cpu: '{{ env "PROMETHEUS_PROMETHEUS_LIMIT_CPU" | default "300m" }}'
              memory: '{{ env "PROMETHEUS_PROMETHEUS_LIMIT_MEMORY" | default "1526Mi" }}'
            requests:
              cpu: '{{ env "PROMETHEUS_PROMETHEUS_REQUEST_CPU" | default "75m" }}'
              memory: '{{ env "PROMETHEUS_PROMETHEUS_REQUEST_MEMORY" | default "768Mi" }}'
      alertmanager:
        enabled: {{ env "ALERTMANAGER_INSTALLED" | default "true" }}
        ## Alertmanager configuration directives
        ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
        ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
        ##
        config:
          global:
            resolve_timeout: 5m
          route:
            group_by:
              - "alertname"
              - "namespace"
            group_wait: 30s
            group_interval: 5m
            repeat_interval: 12h
            receiver: 'general'
            routes:
              - match:
                  alertname: Watchdog
                receiver: 'null'
          receivers:
            - name: 'null'
            - name: "general"
              {{- if not ( env "KUBE_PROMETHEUS_ALERT_MANAGER_SLACK_WEBHOOK_URL" | empty ) }}
              slack_configs:
                ### Required: KUBE_PROMETHEUS_ALERT_MANAGER_SLACK_WEBHOOK_URL
                - api_url: '{{ env "KUBE_PROMETHEUS_ALERT_MANAGER_SLACK_WEBHOOK_URL" }}'
                  ### Required: KUBE_PROMETHEUS_ALERT_MANAGER_SLACK_CHANNEL; e.g. #alerts-staging
                  channel: '{{ env "KUBE_PROMETHEUS_ALERT_MANAGER_SLACK_CHANNEL" }}'
                  send_resolved: true
              {{- end }}
              {{- if not ( env "KUBE_PROMETHEUS_ALERT_MANAGER_PAGERDUTY_INTEGRATION_KEY" | empty ) }}
              pagerduty_configs:
                # In PagerDuty, under "Service Details" for a service, on the Integrations tab,
                # Select a new integration of type Events API v2 and use that key as the integration key.
                # Do NOT select "Prometheus" as the integration type.
                - routing_key: '{{ env "KUBE_PROMETHEUS_ALERT_MANAGER_PAGERDUTY_INTEGRATION_KEY" }}'
                  send_resolved: true
                  details:
                    firing: '{{"{{"}} template "pagerduty.default.firing" . {{"}}"}}'
                    resolved: '{{"{{"}} template "pagerduty.default.resolved" . {{"}}"}}'
              {{- end }}
          templates:
            - ./*.tmpl
        alertmanagerSpec:
          externalUrl: "{{- env "PROMETHEUS_ALERTMANAGER_EXTERNAL_URL" | default (print "https://api." (env "KOPS_CLUSTER_NAME") "/api/v1/namespaces/monitoring/services/prometheus-operator-alertmanager:web/proxy/") }}"
          {{- if env "PROMETHEUS_ALERTMANAGER_STORAGE_SIZE" }}
          storage:
            volumeClaimTemplate:
              spec:
                {{- if env "PROMETHEUS_ALERTMANAGER_STORAGE_CLASS" }}
                storageClassName: {{ env "PROMETHEUS_ALERTMANAGER_STORAGE_CLASS" }}
                {{- end }}
                resources:
                  requests:
                    storage: {{ env "PROMETHEUS_ALERTMANAGER_STORAGE_SIZE" }}
          {{- end }}
          resources:
            limits:
              cpu: '{{ env "PROMETHEUS_ALERTMANAGER_LIMIT_CPU" | default "200m" }}'
              memory: '{{ env "PROMETHEUS_ALERTMANAGER_LIMIT_MEMORY" | default "96Mi" }}'
            requests:
              cpu: '{{ env "PROMETHEUS_ALERTMANAGER_REQUEST_CPU" | default "10m" }}'
              memory: '{{ env "PROMETHEUS_ALERTMANAGER_REQUEST_MEMORY" | default "24Mi" }}'
      grafana:
        # https://github.com/helm/charts/tree/dfa02f9b117a29de889f9c35e0b3abb6012a0877/stable/grafana#configuration
        enabled: {{ env "GRAFANA_INSTALLED" | default "true" }}
        {{- if env "GRAFANA_IMAGE_TAG" }}
        image:
          tag: {{ env "GRAFANA_IMAGE_TAG" }}
        {{- end }}
        adminPassword: {{ env "GRAFANA_ADMIN_PASSWORD" | default "prom-operator-CHANGEME" }}
        defaultDashboardsEnabled: {{ env "PROMETHEUS_GRAFANA_DEFAULT_DASHBOARDS_ENABLED" | default "true" }}
        sidecar:
          dashboards:
            enabled: true
            searchNamespace: ALL
            defaultFolderName: "AutoLoaded"
        {{- if env "GRAFANA_IAM_ROLE" }}
        podAnnotations:
          iam.amazonaws.com/role: {{ env "GRAFANA_IAM_ROLE" }}
        {{- end }}
        plugins:
          - grafana-piechart-panel
        resources:
          limits:
            cpu: '{{ env "GRAFANA_LIMIT_CPU" | default "250m" }}'
            memory: '{{ env "GRAFANA_LIMIT_MEMORY" | default "128Mi" }}'
          requests:
            cpu: '{{ env "GRAFANA_REQUEST_CPU" | default "25m" }}'
            memory: '{{ env "GRAFANA_REQUEST_MEMORY" | default "72Mi" }}'
        grafana.ini:
          dataproxy:
            # default is 30 seconds
            timeout: '{{ env "GRAFANA_PROXY_TIMEOUT" | default "90" }}'
          server:
            # root_url: "https://api.{{- env "KOPS_CLUSTER_NAME" }}/api/v1/namespaces/kube-system/services/prometheus-operator-grafana:service/proxy/"
            root_url: "{{- env "PROMETHEUS_GRAFANA_ROOT_URL" | default (print "https://api." (env "KOPS_CLUSTER_NAME") "/api/v1/namespaces/monitoring/services/prometheus-operator-grafana:service/proxy/") }}"
          {{- if eq (env "PROMETHEUS_ANONYMOUS_ADMIN_ENABLED" | default "true") "true" }}
          auth.anonymous:
            enabled: true
            org_role: Admin
          {{- end }}
          {{- if eq (env "PROMETHEUS_KEYCLOAK_GATEKEEPER_ENABLED" | default "false") "true" }}
          auth:
            oauth_auto_login: true
            disable_signout_menu: true
          users:
            auto_assign_org: true
            auto_assign_org_id: 1
            auto_assign_org_role: '{{ env "PROMETHEUS_KEYCLOAK_GATEKEEPER_AUTO_ROLE" | default "Admin" }}'
          auth.generic_oauth:
            enabled: true
            allow_sign_up:  true
            scopes:  openid profile email
            client_id:  '{{- coalesce (env "KEYCLOAK_GATEKEEPER_CLIENT_ID") (env "KOPS_OIDC_CLIENT_ID") "kubernetes" }}'
            client_secret:  '{{- requiredEnv "KEYCLOAK_GATEKEEPER_CLIENT_SECRET" }}'
            auth_url:  '{{- coalesce (env "KEYCLOAK_GATEKEEPER_DISCOVERY_URL") (env "KOPS_OIDC_ISSUER_URL") }}/protocol/openid-connect/auth'
            token_url:  '{{- coalesce (env "KEYCLOAK_GATEKEEPER_DISCOVERY_URL") (env "KOPS_OIDC_ISSUER_URL") }}/protocol/openid-connect/token'
            api_url:  '{{- coalesce (env "KEYCLOAK_GATEKEEPER_DISCOVERY_URL") (env "KOPS_OIDC_ISSUER_URL") }}/protocol/openid-connect/userinfo'
          {{- end }}
          {{- if env "GRAFANA_DB_HOST" }}
          # We are only supporting MySQL at this time
          database:
            type: mysql
            host: '{{- env "GRAFANA_DB_HOST" -}} : {{- env "GRAFANA_DB_PORT" | default "3306" -}}'
            name: '{{- env "GRAFANA_DB_NAME" | default "grafana" -}}'
            user: '{{- env "GRAFANA_DB_USER" | default "grafana" -}}'
            password: '"""{{- env "GRAFANA_DB_PASSWORD" -}}"""'
            ssl_mode: skip-verify
            # Should not need this, but Grafana requires some certificate be
            # present, even with ssl_mode: skip-verify
            ca_cert_path: /etc/ssl/certs/Amazon_Root_CA_4.pem
            log_queries: {{ env "GRAFANA_LOG_QUERIES" | default "false" }}
          {{- end }}
      kubeStateMetrics:
        enabled: true
      {{- if or (env "PROMETHEUS_OPERATOR_KSM_IMAGE_REPOSITORY") (env "PROMETHEUS_OPERATOR_KSM_IMAGE_TAG") }}
      kube-state-metrics:
        image:
          {{- if env "PROMETHEUS_OPERATOR_KSM_IMAGE_REPOSITORY" }}
          repository: {{ env "PROMETHEUS_OPERATOR_KSM_IMAGE_REPOSITORY" }}
          {{- end }}
          {{- if env "PROMETHEUS_OPERATOR_KSM_IMAGE_TAG" }}
          tag: {{ env "PROMETHEUS_OPERATOR_KSM_IMAGE_TAG" }}
          {{- end }}
      {{- end }}
      kubeApiServer:
        enabled: true
      kubelet:
        enabled: true
        # In general, few clusters are set up to allow kublet to authenticate a bearer token, and
        # the HTTPS endpoint requires authentication, so Prometheus cannot access it.
        # The HTTP endpoint does not require authentication, so Prometheus can access it.
        # See https://github.com/coreos/prometheus-operator/issues/926
        serviceMonitor:
          https: {{ env "PROMETHEUS_KUBELET_HTTPS" | default "false" }}
      kubeControllerManager:
        enabled: true
        {{- if eq (env "PROMETHEUS_KUBE_K8S_APP_LABEL_NAME" | default "false") "true" }}
        service:
          selector:
            k8s-app: "kube-controller-manager"
            component: null
        {{- end }}
      coreDns:
        enabled: {{ coalesce (env "PROMETHEUS_EXPORTER_CORE_DNS_ENABLED") "true" }}
      kubeDns:
        enabled: {{ coalesce (env "PROMETHEUS_EXPORTER_KUBE_DNS_ENABLED") "false" }}
      kubeEtcd:
        # Access to etcd is a huge security risk, so nodes are blocked from accessing it.
        # Therefore Prometheus cannot access it without extra setup, which is beyond the scope of this helmfile.
        # See https://github.com/kubernetes/kops/issues/5852
        #     https://github.com/kubernetes/kops/issues/4975#issuecomment-381055946
        #     https://github.com/coreos/prometheus-operator/issues/2397
        #     https://github.com/coreos/prometheus-operator/blob/v0.19.0/contrib/kube-prometheus/docs/Monitoring%20external%20etcd.md
        #     https://gist.github.com/jhohertz/476bd616d4171649a794b8c409f8d548
        # So we disable it since it is not going to work anyway
        enabled: false
      kubeScheduler:
        enabled: true
        {{- if eq (env "PROMETHEUS_KUBE_K8S_APP_LABEL_NAME" | default "false") "true" }}
        service:
          selector:
            k8s-app: "kube-scheduler"
            component: null
        {{- end }}
      nodeExporter:
        enabled: true
    {{- if env "PROMETHEUS_ADDITIONAL_CONFIG_YAML" }}
    - {{ env "PROMETHEUS_ADDITIONAL_CONFIG_YAML" }}
    {{ end }}
    set:
    - name: "alertmanager.templateFiles.deployment\\.tmpl"
      file: ./values/kube-prometheus.alerts.template
