## Teleport Auth chart configuration

{{- $authurl := env "TELEPORT_AUTH_DOMAIN_NAME" | default (print "auth." (requiredEnv "TELEPORT_PROXY_DOMAIN_NAME")) }}

extraArgs:
  ## enable debug
  # debug:

podAnnotations:
  iam.amazonaws.com/role: "{{ requiredEnv "TELEPORT_AUTH_IAM_ROLE" }}"

service:
  annotations:
     external-dns.alpha.kubernetes.io/hostname: "{{ $authurl }}."

## teleport config file
config:
  teleport.yaml: |
    # This section of the configuration file applies to all teleport services
    teleport:
      # nodename allows to assign an alternative name this node can be reached by
      # by default it's equal to hostname
      #nodename:

      # Data directory where Teleport keeps its data, like keys/users for
      # authentication (if using the default BoltDB back-end)
      data_dir: /var/lib/teleport

      # one-time invitation token used to join a cluster. it is not used on
      # subsequent starts
      auth_token: "{{ env "TELEPORT_AUTH_TOKEN" | default "auth-not-completely-secure-but-not-a-huge-risk" }}"

      # when running in multi-homed or NATed environments Teleport nodes need
      # to know which IP it will be reachable at by other nodes
      # advertise_ip: {{/* aws.EC2Meta "local-ipv4" */}}

      # Teleport throttles all connections to avoid abuse. These settings allow
      # you to adjust the default limits
      connection_limits:
        # Number of max. simultaneous connections from an IP
        max_connections: 1000
        # Number of max. simultaneous connected users/logins
        max_users: 100

      # Logging configuration. Possible output values are 'stdout', 'stderr' and
      # 'syslog'. Possible severity values are INFO, WARN and ERROR (default).
      log:
        output: stdout
        severity: INFO

      # Type of storage used for keys. You need to configure this to use etcd
      # backend if you want to run Teleport in HA configuration.
      storage:
        type: dynamodb
        # audit_sessions_uri: s3://bucket-name
        audit_sessions_uri: {{ requiredEnv "TELEPORT_AUDIT_SESSIONS_URI" }}
        # audit_events_uri: dynamodb://table-name
        audit_events_uri: {{ requiredEnv "TELEPORT_AUDIT_EVENTS_URI" }}
        region: {{ requiredEnv "AWS_REGION" }}
        table_name: {{ requiredEnv "TELEPORT_CLUSTER_STATE_DYNAMODB_TABLE" }}
      #storage:
      #  type: dir
      #  audit_events_uri: file:///tmp/teleport/events.log
      #  audit_sessions_uri: file:///tmp/teleport/sessions.log

    # This section configures the 'auth service':
    auth_service:
      # Turns 'auth' role on. Default is 'yes'
      enabled: yes

      # defines the types and second factors the auth server supports
      authentication:
        # type can be local or oidc or saml
{{- if eq (env "TELEPORT_USE_SAML" | default "false") "true" }}
        type: saml
        second_factor: off
{{- else }}
        type: local
        # second_factor can be off, otp, or u2f
        second_factor: {{ env "TELEPORT_LOCAL_SECOND_FACTOR" | default "off" }}

        # this section is only used if using u2f
        u2f:
          # app_id should point to the Web UI.
          app_id: https://0.0.0.0

          # facets should list all proxy servers.
          facets:
          - https://0.0.0.0
          - https://0.0.0.0:3080
{{- end }}

      # Determines if SSH sessions to cluster nodes are forcefully terminated
      # after no activity from a client (idle client).
      # Examples: "30m", "1h" or "1h30m"
      client_idle_timeout: 15m

      # IP and the port to bind to. Other Teleport nodes will be connecting to
      # this port (AKA "Auth API" or "Cluster API") to validate client
      # certificates
      listen_addr: 0.0.0.0:3025

      # The optional DNS name the auth server if located behind a load balancer.
      public_addr: {{ $authurl }}

      # Pre-defined tokens for adding new nodes to a cluster. Each token specifies
      # the role a new node will be allowed to assume. The more secure way to
      # add nodes is to use `ttl node add --ttl` command to generate auto-expiring
      # tokens.
      #
      # We recommend to use tools like `pwgen` to generate sufficiently random
      # tokens of 32+ byte length.
      tokens:
        - node:{{ env "TELEPORT_NODE_TOKEN" | default "node-not-completely-secure-but-not-a-huge-risk" }}
        - proxy:{{ env "TELEPORT_PROXY_TOKEN" | default "proxy-not-completely-secure-but-not-a-huge-risk" }}
        - auth:{{ env "TELEPORT_AUTH_TOKEN" | default "auth-not-completely-secure-but-not-a-huge-risk" }}
        - trusted_cluster:{{ env "TELEPORT_CLUSTER_TOKEN" | default "cluster-not-completely-secure-but-not-a-huge-risk" }}

      # Optional "cluster name" is needed when configuring trust between multiple
      # auth servers. A cluster name is used as part of a signature in certificates
      # generated by this CA.
      #
      # By default an automatically generated GUID is used.
      #
      # IMPORTANT: if you change cluster_name, it will invalidate all generated
      # certificates and keys (may need to wipe out /var/lib/teleport directory)
      cluster_name: "{{ coalesce (env "TELEPORT_CLUSTER_NAME") (env "STAGE") (requiredEnv "TELEPORT_PROXY_DOMAIN_NAME") }}"

      # License file to start auth server with. Note that this setting is ignored
      # in open-source Teleport and is required only for Teleport Pro, Business
      # and Enterprise subscription plans.
      #
      # The path can be either absolute or relative to the configured `data_dir`
      # and should point to the license file obtained from Teleport Download Portal.
      #
      license_file: "/etc/teleport/license.pem"

  ## License file
  license.pem: |
{{ requiredEnv "TELEPORT_LICENSE_PEM" | trim | indent 4 }}

bootstrap:
  enabled: true
  resources:

## Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

# Tolerations for pod assignment
# Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []
