repositories:
  # Cloud Posse incubator repo of helm charts
  - name: "cloudposse-incubator"
    url: "https://charts.cloudposse.com/incubator/"

releases:

  #######################################################################################
  ## Fluentd to AWS Elasticsearch                                                      ##
  ## Forward logs to AWS Elasticsearch with fluentd                                    ##
  #######################################################################################

  #
  # References:
  #   - https://github.com/cloudposse/charts/blob/master/incubator/fluentd-kubernetes-aws/values.yaml
  #

  - name: "fluentd-elasticsearch-aws"
    namespace: "monitoring"
    labels:
      chart: "fluentd-kubernetes-aws"
      repo: "cloudposse-incubator"
      component: "fluentd"
      namespace: "monitoring"
      default: "false"
    chart: "cloudposse-incubator/fluentd-kubernetes-aws"
    version: "0.2.1"
    wait: true
    installed: {{ env "FLUENTD_ELASTICSEARCH_AWS_INSTALLED" | default "true" }}
    values:
      - nameOverride: "fluentd-elasticsearch-aws"
        image:
          repository: '{{ env "FLUENTD_ELASTICSEARCH_IMAGE_REPO" | default "fluent/fluentd-kubernetes-daemonset" }}'
          tag: '{{ env "FLUENTD_ELASTICSEARCH_IMAGE_TAG" | default "v1.3.3-debian-elasticsearch-1.8" }}'
          pullPolicy: '{{ env "FLUENTD_ELASTICSEARCH_IMAGE_PULL_POLICY" | default "IfNotPresent" }}'
        # Leave role empty if you are not uisng IAM based access control
        role: '{{ env "FLUENTED_ELASTICSEARCH_IAM_ROLE" | default "" }}'
        elasticsearch:
          endpoint: '{{ requiredEnv "FLUENTD_ELASTICSEARCH_ENDPOINT" }}'
          region: '{{ coalesce (env "FLUENTD_ELASTICSEARCH_REGION") (env "AWS_REGION") (env "AWS_DEFAULT_REGION") }}'

        prometheus:
          createService: {{ env "FLUENTD_PROMETHEUS_CREATE_SERVICE" | default "true" }}
          createServiceMonitor: {{ env "FLUENTD_PROMETHEUS_CREATE_SERVICE_MONITOR" | default "true" }}
          createRule: {{ env "FLUENTD_PROMETHEUS_CREATE_RULE" | default "true" }}
          labels:
            app: prometheus-operator
            release: prometheus-operator
        env:
          # The AWS Elasticsearch server (at with least the smaller machines) has a hard limit on request size of 10 MB.
          # When sending bulk updates, such as catching up on old logs after a log forwarding outage, requests can get
          # substantially bigger than this.
          # As of 3.5.1, the fluentd-plugin-elasticearch has a configurable threshold, and when the potential bulk
          # request size gets above the threshold, it will start breaking down the request into smaller requests. However,
          # there is no guarantee that the decomposed requests will themselves be smaller than the threshold. So we need
          # to set both the threshold and the maximum chunck size to smaller numbers that guarantee the behavior of the
          # algorithm is such that in the end the 10 MB limit is not reached.
          # See https://github.com/uken/fluent-plugin-elasticsearch/issues/588
          FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE: '{{- env "FLUENTD_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE" | default "4M" }}'
          FLUENT_ELASTICSEARCH_BULK_MESSAGE_REQUEST_THRESHOLD: '{{- env "FLUENTD_ELASTICSEARCH_BULK_MESSAGE_REQUEST_THRESHOLD" | default "4M" }}'
          FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH: '{{- env "FLUENTD_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH" | default "64" }}'
          FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL: '{{- env "FLUENTD_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL" | default "5s" }}'
          # Reload Connections must be false for AWS
          # https://github.com/atomita/fluent-plugin-aws-elasticsearch-service/commit/3289962dd4d6c5de7996c618eb6edc5f769174d8
          FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS: false
          FLUENT_ELASTICSEARCH_RECONNECT_ON_ERROR: true
          FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE: true
          FLUENT_ELASTICSEARCH_SSL_VERSION: "TLSv1_2"
          # Settings for log throttling.
          # See https://github.com/rubrikinc/fluent-plugin-throttle
          FLUENTD_THROTTLE_GROUP_KEY: '{{- env "FLUENTD_THROTTLE_GROUP_KEY" | default "kubernetes.pod_name" }}'
          FLUENTD_THROTTLE_GROUP_BUCKET_PERIOD: '{{- env "FLUENTD_THROTTLE_GROUP_BUCKET_PERIOD" | default "3" }}'
          FLUENTD_THROTTLE_GROUP_BUCKET_LIMIT: '{{- env "FLUENTD_THROTTLE_GROUP_BUCKET_LIMIT" | default "300" }}'
          FLUENTD_THROTTLE_GROUP_DROP_LOGS: '{{- env "FLUENTD_THROTTLE_GROUP_DROP_LOGS" | default "false" }}'
          FLUENTD_THROTTLE_GROUP_RESET_RATE: '{{- env "FLUENTD_THROTTLE_GROUP_RESET_RATE" | default "-1" }}'
          FLUENTD_THROTTLE_GROUP_WARNING_DELAY: '{{- env "FLUENTD_THROTTLE_GROUP_WARNING_DELAY" | default "10" }}'

        resources:
          limits:
            cpu: '{{- env "FLUENTD_ELASTICSEARCH_RESOURCES_LIMIT_CPU" | default "100m" }}'
            memory: '{{- env "FLUENTD_ELASTICSEARCH_RESOURCES_LIMIT_MEMORY" | default "512Mi" }}'
          requests:
            cpu: '{{- env "FLUENTD_ELASTICSEARCH_RESOURCES_REQUEST_CPU" | default "20m" }}'
            memory: '{{- env "FLUENTD_ELASTICSEARCH_RESOURCES_REQUEST_MEMORY" | default "256Mi" }}'

  - name: 'fluentd-throttle-alerts'
    namespace: "monitoring"
    chart: "cloudposse-incubator/monochart"
    version: "0.18.4"
    wait: true
    force: true
    installed: {{ env "FLUENTD_THROTTLE_ALERTS_INSTALLED" | default "false" }}
    values:
      - prometheusRules:
          name:
            labels:
              app: prometheus-operator-prometheus
            groups:
              - name: fluentd-throttle.rules
                rules:
                  - alert: FluentdThrottleActivated
                    annotations:
                      message: Fluentd is throttling messages for pod {{`{{ $labels.group_key }}`}}
                      description: Throttling of logs undesirable behaviour and indicates problem with related application. Throttling is enabled to mitigate possibility of choking log system.
                    expr: sum(increase(fluentd_events_rate_exceeded_messages_total[1m])) by (group_key) > 0
                    for: 15m
                    labels:
                      severity: error
